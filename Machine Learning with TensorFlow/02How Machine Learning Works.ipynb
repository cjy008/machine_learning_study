{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions \n",
    "- **neural network**: computational graph, a network of interconnected nodes through which data flows. Begins with input nodes, has some number of hidden or intermediate nodes, and ends with output nodes\n",
    "- **node**: often called a neuron, a point in a neural network through which data flows, typically has weights, biases, and an activation function. Input & output nodes simply act as an entry and exit points for the data, often connected to all nodes in the previous and next layers.\n",
    "- **weights**: factor by which inputs are multiplies, each input to a node has a weight; these values are modified during training\n",
    "- **bias**: number added to the product of weights and inputs, provides a way to ensure neurons are passing on outputs; prevents breakage in the network\n",
    "- **weighted sum**: multiply each input by the corresponding weight, add the bias and sum all of these results up\n",
    "- **activation functions**: function applied to the weighted sum to transform output to a value that indicates whether or not a neuron fires or passes on its value. Usually there are two types:\n",
    "    - **ReLu**: replace Sigmoid and TanH functions as it solves the vanishing gradient problem (nvalues approaching 0), if input < 0, output = 0; if input > 0, output is unmodified input\n",
    "    - **TanH**: Inputs produce output between -1 and 1\n",
    "    - **Sigmoid**: Inputs produce output between 0 and 1\n",
    "    - **Softmax**: typically used in the final layer of probability and classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common ML structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Feed Foward\n",
    "- Inputs are modified through an activation function to feed directly to the output nodes\n",
    "- No extra hidden layers\n",
    "- Often called a perceptron\n",
    "- Often trained through **delta rule** algorithm; calculate the difference between the expected and actual output and adjust weights in order to minimize difference, form of gradient descent (smaller difference = closer to correct answer)\n",
    "- Supervised learning\n",
    "<br>\n",
    "\n",
    "- **Example**: image recognition and classification (1 input, 1 hidden and 1 output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Feed Forward\n",
    "- More complex version of a perceptron with multiple hidden layers (at least 2) of interconnected nodes (each node is connected to every node in the next layer)\n",
    "- Most common algorithm is **back-propagation**: similar to perceptron, compares actual and expected outputs and produces an error based on the total differences, then adjust weights and runs again during training\n",
    "- Objective: minimize error and it does so using a non-linear gradient descent optimizer\n",
    "- Supervised learning\n",
    "- Better at complex problems; but slower with many hidden layers\n",
    "<br>\n",
    "\n",
    "- **Example**: Image recognition and classification (multiple hidden layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF)\n",
    "- Structured similar to perceptron but has a hidden layer with neurons with radial basis activation functions\n",
    "- Activation functions are **Gaussian**: neurons fire maximally when distance between weights are similar to inputs\n",
    "- Excellent at detecting anomalies but not so good at extrapolation\n",
    "- Makes them good at classification problems\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "- Structured similar to perceptron but has hidden layer(s) with convolution functions and pooling functions\n",
    "- Helps transform inputs into smaller inputs\n",
    "- Convolution functions create a complex pattern by processing smaller, less complex units (baby steps)\n",
    "- Excellent as image recognition and classification\n",
    "- Can be prone to **overfitting**: deducing patterns when there are none and losing sight of the actual pattern we are trying to find (playing tricks on ourselves) which gives the model an overconfidence\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "- No longer feed-forward, hidden layers often generally contain LSTM cells\n",
    "- LSTM cells retain some memory or state and output is dependent on current input and current state\n",
    "- Act somewhat similar to a cyclical multi-layer perceptron\n",
    "- Excellent at anything text or speech related, especially when inputs and outputs are different lengths\n",
    "- Supervised or reinforcement learning (depending on how you build the network)\n",
    "<br>\n",
    "\n",
    "- **Example**: Speech Recognition and language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular Neural Network\n",
    "- separated into two or more independent modules and managed by an intermediary\n",
    "- modules process input separately and do not interact, usually performs a specific task\n",
    "- Intermediary takes outputs from each module and puts them together without modifying them\n",
    "- Can be efficient, better at each individual task and less prone to failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model\n",
    "- Network in 3 parts: encoder, intermediary, and decoder\n",
    "- Encodes all inputs in some format, often a map assigning a value to each possible part of an input\n",
    "- Decodes encoded output, typically using an RNN, to some readable output\n",
    "- Excellent at producing a different kind of input from output or when working with outputs of different lengths\n",
    "- Supervised or reinforcement learning\n",
    "- Good at language translation\n",
    "<br>\n",
    "\n",
    "-**Example**: image captioning, language translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
