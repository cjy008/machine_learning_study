{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is image recognition?\n",
    "- Image recognition is seeing an object or an image of an object and knowing what it is\n",
    "- You classify everything that you see into a certain category based on attributes (Attributes depend on the set of data you are classifying)\n",
    "- Even if you see something you have never seen before, you can usually place it in some category\n",
    "    - Example: a brand new model of car, you can still tell its a car by the wheels, hood, windows, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this work for humans?\n",
    "- A lot of the time, image recognition for us happens subconsciously\n",
    "- We don't necessarily acknowledge everything that is around us\n",
    "- However, when we need to notice something, we can usually pick it out and define and describe it\n",
    "- Knowing what something is is based entirely on previous experiences\n",
    "- Some things we memorize, others we deduce based on shared characteristics that we see in things we do know\n",
    "- Subconsciously we separate the items we see based on borders defined primarily by differences in color\n",
    "    - Example: It is easy to see a green leaf on a brown tree, but hard to see a black cat against a black wall\n",
    "- We don't necessarily need to look at every part of an image to know what some part of it is\n",
    "    - Example: If we only see an eye and an ear of someone's face, we know it is a face "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work for machines? \n",
    "- Machines do not have infinite knowledge of what everything they see\n",
    "- They only have knowledge of the categories we have taught them\n",
    "    - Example: If you create facial recognition, it only classifies images into faces or not faces\n",
    "- Even the most sophisticated image recognition models, cannot recognize everything and have been trained only to look for certain objects\n",
    "- To a machine, an image is an array of bytes\n",
    "- Each pixel on an image contains information about the RGB values\n",
    "- If an image is black/white, the value for each pixel is simply a darkness value ( 255 = white and 0 = black)\n",
    "- Machines do not care about seeing an image as a whole\n",
    "- To process an image, they simply look at the values for each of the bytes and look for patterns\n",
    "- Image recognition models look for groups of similar byte values across images to place an image in a specific category\n",
    "    - Example: high green and brown values in adjacent bytes might suggest an image contains a tree. If many images all have similar groupings of green and brown values, the model may think they all contain trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What tools can help us to solve it?\n",
    "- Processing an image takes a lot of time; most images fed into simple models are small (MNIST images are 28 x 28 pixels = 784 pixels)\n",
    "- It is difficult to recognize consistent patterns when comparing all 784 pixel values of each image to another\n",
    "- Slight position, size changes, or slightly different shapes can lead to mislabelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Machines Solve This\n",
    "- Machines solve this problem by first breaking down images into smaller parts and processing them instead\n",
    "- It starts by applying a sort of a filter to different parts of the image a few pixels at a time to produce several smaller, distorted images\n",
    "- Afterwards, it makes the pieces more abstract by averaging together smaller squares of pixel values\n",
    "- This ultimately turns the image into something that may not be recognizable by humans but the machines can make sense of it (black and white blobs for it to read the patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "- The process of breaking down images and applying the filters are done in the **convolutional** layer in a neural network\n",
    "- Averaging the values to further distort images is done through a **max pooling layer**\n",
    "- **Convolutional Neural Networks** get their names from the fact that they have one or more convolutional layers\n",
    "- These are very popular in image recognition models, although RRNs have also performed well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "- **Convolution**: an operation on two functions to produce a third function that explains how the shape of one is modified by the other\n",
    "- **Image Convolution**: applying a **kernel** or **convolution mask** to blocks of pixels to apply an effect\n",
    "- Often used to blur or sharpen images, or detect edges\n",
    "- A **kernel** is a matrix used as a mask to image pixel values\n",
    "- Kernels are often pre-determined through Tensorflow objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "- **Max Pooling**: replacing a block of pixels by one pixel with the highest value out of the block\n",
    "- Makes the image more abstract as it reduces the noise and simplifies the images\n",
    "- Used to downsize an image and also prevent overfitting (drawing conclusions when there are none)\n",
    "- Generally, we specify the size of the matrix and the step size (# of pixels to skip over when applying the next max pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Almost all convolutional neural networks will have a convolutional layer followed by a max pooling layer\n",
    "- Larger networks will repeat this one or more times along with other layers to perform additional processing\n",
    "- The result of the two layers is a set of smaller, more abstract images comprised of parts of the original image\n",
    "- The purpose is to cut out unnecessary image noise and to focus on the stand-out features so that the model can focus on what is important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
